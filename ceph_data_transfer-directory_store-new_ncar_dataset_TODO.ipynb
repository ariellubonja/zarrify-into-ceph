{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/turbulence-ceph-staging/sciserver-turbulence\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/turbulence-ceph-staging/sciserver-turbulence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import packages\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import zarr\n",
    "import itertools\n",
    "import contextlib\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from zarr.storage import DirectoryStore\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "parameters\n",
    "\"\"\"\n",
    "dataset_title = 'stsabl2048low'\n",
    "store_path = f\"/home/idies/workspace/turbulence-ceph-staging/sciserver-turbulence/{dataset_title}/{dataset_title}.zarr\"\n",
    "store = DirectoryStore(store_path, dimension_separator = '/')\n",
    "# time offset from 0 when querying the dataset with giverny.\n",
    "time_offset = 1\n",
    "# offset from the first time chunk to write data into the zarr store on ceph.\n",
    "# if this value = 0, that means time 0 is written to the zarr time chunk folder = 0.\n",
    "# if this value = 1, that means time 0 is written to the zarr time chunk folder = 1. this is needed to keep a placeholder time chunk folder for pchip interpolation\n",
    "# because the precursor time cannot be read by giverny getCutout and will have to be read and written manually.\n",
    "time_ceph_offset = 1\n",
    "# xyz and time dimensions for the full dataset including times for pchip interpolation that cannot be read by giverny.\n",
    "xyzt_dims_full = [2048, 2048, 2048, 20]\n",
    "# xyz and time dimensions for the dataset to be read with giverny.\n",
    "xyzt_dims = [2048, 2048, 2048, 20] \n",
    "# xyz and time chunk sizes for the zarr store on ceph.\n",
    "xyzt_chunk_sizes = [64, 64, 64, 1]\n",
    "# xyz and time dimensions to query in parallel when reading from the legacy stores.\n",
    "xyzt_filedb_file_dims = np.array([512, 512, 512, 1])\n",
    "# use the default stride value of 1 for each axis when using giverny to retrieve a cutout of the data.\n",
    "strides = [1, 1, 1]\n",
    "# map the zarr group variables to the number of values stored for each grid point.\n",
    "zarr_groups = {\n",
    "    'velocity': 3,\n",
    "    'pressure': 1,\n",
    "    'temperature': 1,\n",
    "    'energy': 1\n",
    "}\n",
    "zarr_variables = list(zarr_groups.keys())\n",
    "num_variables = len(zarr_variables)\n",
    "# number of workers to read in parallel.\n",
    "num_workers = 8\n",
    "# maximum number of retries in case of an error.\n",
    "max_retries = 10\n",
    "# output path for writing the report text file.\n",
    "output_path = os.path.join('/home/idies/workspace/turbulence-ceph-staging/sciserver-turbulence/reports/', dataset_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zarr store created.\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create the zarr store\n",
    "\"\"\"\n",
    "def create_zarr_store(store, xyzt_dims_full, xyzt_chunk_sizes, zarr_groups):\n",
    "    dt = np.dtype(np.float32)\n",
    "    dt = dt.newbyteorder('<')\n",
    "    \n",
    "    # create the zarr store.\n",
    "    root = zarr.group(store = store, overwrite = True, synchronizer = None)\n",
    "    \n",
    "    # create the zarr group for each variable.\n",
    "    for variable_name in zarr_groups:\n",
    "        zarr_dims = zarr_groups[variable_name]\n",
    "        \n",
    "        zarr_group = root.zeros(variable_name, shape = (xyzt_dims_full[3], xyzt_dims_full[2], xyzt_dims_full[1], xyzt_dims_full[0], zarr_dims), \n",
    "                                chunks = (xyzt_chunk_sizes[3], xyzt_chunk_sizes[2], xyzt_chunk_sizes[1], xyzt_chunk_sizes[0], zarr_dims), \n",
    "                                dtype = dt, compressor = None)\n",
    "    \n",
    "    print('zarr store created.')\n",
    "    print('-')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "create_zarr_store(store, xyzt_dims_full, xyzt_chunk_sizes, zarr_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in self.location_paths:\n",
    "    self.NCAR_files += glob.glob(os.path.join(path, f'*{self.file_extension}'))\n",
    "\n",
    "timestep = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babd23622ef64234b9ccf18fa64bef75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='chunks completed'), FloatProgress(value=0.0, max=64.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2435f0e7d1ee41cb9fdb7404689ec0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='time'), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrying (n = 8) time = 19                                                                          "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "read JHTDB datasets from fileDB using giverny and then write to a zarr store on ceph\n",
    "\"\"\"\n",
    "import xarray as xr\n",
    "\n",
    "# open the zarr store using DirectoryStore.\n",
    "root = zarr.open(store, mode = 'a')\n",
    "\n",
    "def process_cube(coords):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        TODO : Ariel change code here for reading in the new NCAR data without giverny.\n",
    "        \"\"\"\n",
    "        \n",
    "        def select_file(file_list, timestep):\n",
    "            for full_path in file_list:\n",
    "                # Extract the filename from the full path\n",
    "                filename = os.path.basename(full_path)\n",
    "\n",
    "                # Extract the number from the filename using a more specific regular expression\n",
    "                match = re.search(r'jhd\\.(\\d+)\\.nc', filename)\n",
    "                if match:\n",
    "                    file_timestep = int(match.group(1))\n",
    "                    if file_timestep == timestep:\n",
    "                        return full_path\n",
    "            # If no file is found, raise an exception\n",
    "            raise FileNotFoundError(f\"No file found for timestep {timestep}\")\n",
    "        \n",
    "\n",
    "        # initialize dataset and open zarr store.\n",
    "#         dataset = turb_dataset(dataset_title = dataset_title, output_path = '', auth_token = '')\n",
    "        data_xr = xr.open_dataset(select_file(self.NCAR_files,timestep),\n",
    "                                  chunks={'nnz': xyzt_filedb_file_dims[0], 'nny': xyzt_filedb_file_dims[1],\n",
    "                                          'nnx': xyzt_filedb_file_dims[2]})\n",
    "    \n",
    "        assert isinstance(data_xr['e'].data, dask.array.core.Array)\n",
    "\n",
    "        self.array_cube_side = self._get_data_cube_side(data_xr)\n",
    "\n",
    "        # Add an extra dimension to the data to match isotropic8192 Drop is there to drop the Coordinates object\n",
    "        # that is created - this creates a separate folder when to_zarr() is called\n",
    "        expanded_ds = data_xr.expand_dims({'extra_dim': [1]}).drop_vars('extra_dim')\n",
    "        # The above adds the extra dimension to the start. Fix that - put it in the back\n",
    "        transposed_ds = expanded_ds.transpose('nnz', 'nny', 'nnx', 'extra_dim')\n",
    "\n",
    "        # Group 3 velocity components together\n",
    "        # Never use dask with remote network location on this!!\n",
    "        merged_velocity = write_utils.merge_velocities(transposed_ds, chunk_size_base=self.desired_zarr_chunk_size)\n",
    "\n",
    "        # TODO this is also hard-coded\n",
    "        merged_velocity = merged_velocity.rename({'e': 'energy', 't': 'temperature', 'p': 'pressure'})\n",
    "\n",
    "        dims = [dim for dim in data_xr.dims]\n",
    "        dims.reverse()  # use (nnz, nny, nnx) instead of (nnx, nny, nnz)\n",
    "        \n",
    "        dataset = data_xr\n",
    "\n",
    "        \n",
    "        x, y, z = [coord * xyzt_filedb_file_dims[index] + 1 for index, coord in enumerate(coords[:3])]\n",
    "        time = coords[3]\n",
    "        ranges = [[x, x + xyzt_filedb_file_dims[0] - 1],\n",
    "                  [y, y + xyzt_filedb_file_dims[1] - 1],\n",
    "                  [z, z + xyzt_filedb_file_dims[2] - 1]]\n",
    "            \n",
    "        variable_data = []\n",
    "        for zarr_variable in zarr_variables:\n",
    "            variable_data.append(getCutout(dataset, zarr_variable, time + time_offset, np.array(ranges), np.array(strides), verbose = False).to_array().to_numpy()[0])\n",
    "            \n",
    "        \"\"\"\n",
    "        TODO : End of code for reading in data.\n",
    "        \"\"\"\n",
    "\n",
    "        def save_store(giverny_cube, variable_name):\n",
    "            root[variable_name][time + time_ceph_offset,\n",
    "                                z - 1 : z + xyzt_filedb_file_dims[2] - 1,\n",
    "                                y - 1 : y + xyzt_filedb_file_dims[1] - 1,\n",
    "                                x - 1 : x + xyzt_filedb_file_dims[0] - 1, :] = giverny_cube\n",
    "            \n",
    "        def verify_copy(giverny_cube, variable_name):\n",
    "            ceph_cube = root[variable_name][time + time_ceph_offset,\n",
    "                                            z - 1 : z + xyzt_filedb_file_dims[2] - 1,\n",
    "                                            y - 1 : y + xyzt_filedb_file_dims[1] - 1,\n",
    "                                            x - 1 : x + xyzt_filedb_file_dims[0] - 1, :]\n",
    "            \n",
    "            if np.all(giverny_cube == ceph_cube):\n",
    "                return f\"valid copy\"\n",
    "            else:\n",
    "                return f\"corrupt copy\"\n",
    "\n",
    "        # save the variables in parallel.\n",
    "        with ThreadPoolExecutor(num_variables) as p:\n",
    "            list(p.map(save_store, variable_data, zarr_variables))\n",
    "            \n",
    "        # verify that the copies are not corrupt.\n",
    "        verified = []\n",
    "        with ThreadPoolExecutor(num_variables) as p:\n",
    "            verified = list(p.map(verify_copy, variable_data, zarr_variables))\n",
    "\n",
    "        if any([message == \"corrupt copy\" for message in verified]) or len(verified) != num_variables:\n",
    "            return f\"error processing cube at {coords}: verification failed\"\n",
    "        else:\n",
    "            return f\"successfully processed cube at {coords}\"\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"error processing cube at {coords}: {type(e)}, {str(e)}\")\n",
    "        \n",
    "# Create a context manager to suppress stderr\n",
    "@contextlib.contextmanager\n",
    "def suppress_stderr():\n",
    "    with open(os.devnull, 'w') as devnull:\n",
    "        with contextlib.redirect_stderr(devnull):\n",
    "            yield\n",
    "\n",
    "# write the report file.\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "# process cubes in parallel.\n",
    "cube_coords = list(itertools.product(*[range(dim // chunk) for dim, chunk in zip(xyzt_dims[:3], xyzt_filedb_file_dims[:3])]))\n",
    "    \n",
    "current_time_pbar = tqdm(total = len(cube_coords), desc = \"chunks completed\", leave = False)\n",
    "    \n",
    "with open(output_path + f\"{dataset_title}_report-success.txt\", \"w\") as f_success:\n",
    "    with open(output_path + f\"{dataset_title}_report-error.txt\", \"w\") as f_error:\n",
    "        for time in tqdm(range(xyzt_dims[-1]), total = len(range(xyzt_dims[-1])), desc = \"time\"):\n",
    "            correct_flag = False\n",
    "            retries = 0\n",
    "            while not correct_flag and retries <= max_retries:\n",
    "                retries += 1\n",
    "                \n",
    "                # update cube_coords with the time.\n",
    "                cube_coords_time = [(x, y, z, time) for x, y, z in cube_coords]\n",
    "                \n",
    "                results = []\n",
    "                try:\n",
    "                    with ProcessPoolExecutor(num_workers) as executor:\n",
    "                        with suppress_stderr():\n",
    "                            for result in executor.map(process_cube, cube_coords_time):\n",
    "                                results.append(result)\n",
    "                                current_time_pbar.update(1)\n",
    "                                \n",
    "                                if \"error processing cube\" in result:\n",
    "                                    break\n",
    "                    \n",
    "                    # reset the chunk progress bar.\n",
    "                    current_time_pbar.reset()\n",
    "                    \n",
    "                    print('\\r' + ' ' * 100 + '\\r', end = '', flush = True)\n",
    "                    if any([\"error processing cube\" in message for message in results]) or results == []:\n",
    "                        if retries > max_retries:\n",
    "                            output_str = f\"error consolidating data for time = {time}\\n\"\n",
    "                            f_error.write(output_str)\n",
    "                            f_error.flush()\n",
    "\n",
    "                            print(f\"\\rerror consolidating data for time = {time}\", end = '')\n",
    "                        else:\n",
    "                            print(f\"\\rretrying (n = {retries}) time = {time}\", end = '')\n",
    "                            continue\n",
    "                    else:\n",
    "                        output_str = f\"successfully consolidated data for time = {time}\\n\"\n",
    "                        f_success.write(output_str)\n",
    "                        f_success.flush()\n",
    "\n",
    "                        print(f\"\\rsuccessfully consolidated data for time = {time}\", end = '')\n",
    "\n",
    "                        correct_flag = True\n",
    "                except:\n",
    "                    print('\\r' + ' ' * 100 + '\\r', end = '', flush = True)\n",
    "                    \n",
    "                    # reset the chunk progress bar.\n",
    "                    current_time_pbar.reset()\n",
    "                    \n",
    "                    if retries > max_retries:\n",
    "                        output_str = f\"code exception when consolidating data for time = {time}\\n\"\n",
    "                        f_error.write(output_str)\n",
    "                        f_error.flush()\n",
    "\n",
    "                        print(f\"\\rcode exception when consolidating data for time = {time}\", end = '')\n",
    "                    else:\n",
    "                        print(f\"\\rretrying (n = {retries}) time = {time}\", end = '')\n",
    "                        continue\n",
    "\n",
    "print('\\n-')\n",
    "print(\"completed zarr consolidation, check report files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./reports/stsabl2048low/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/turbulence-ceph-staging/sciserver-turbulence\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel5200\t\t iso8192\t\t rotstrat4096\n",
      "contour_temp-test.ipynb  isotropic1024coarse\t sabl2048high\n",
      "giverny_transfer\t isotropic1024coarseSQL  sabl2048low\n",
      "highspeed_bl\t\t isotropic1024fine\t stsabl2048low\n",
      "iso32768\t\t isotropic8192\n",
      "iso4096\t\t\t mixing\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (py38)",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
