{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2a96b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:38:27.971780Z",
     "iopub.status.busy": "2024-11-06T00:38:27.970544Z",
     "iopub.status.idle": "2024-11-06T00:38:27.983062Z",
     "shell.execute_reply": "2024-11-06T00:38:27.981419Z",
     "shell.execute_reply.started": "2024-11-06T00:38:27.971726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/turbulence-ceph-staging/sciserver-turbulence\n"
     ]
    }
   ],
   "source": [
    "%cd /home/idies/workspace/turbulence-ceph-staging/sciserver-turbulence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915ec1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:38:28.111242Z",
     "iopub.status.busy": "2024-11-06T00:38:28.110506Z",
     "iopub.status.idle": "2024-11-06T00:38:28.123563Z",
     "shell.execute_reply": "2024-11-06T00:38:28.121666Z",
     "shell.execute_reply.started": "2024-11-06T00:38:28.111188Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "NCAR_JHF_BASE_PATH = '/home/idies/workspace/turbulence-ceph-staging/ncar-jhf'\n",
    "\n",
    "NCAR_JHF_HR_PATH = os.path.join(NCAR_JHF_BASE_PATH, 'hr')\n",
    "NCAR_JHF_LR_PATH = os.path.join(NCAR_JHF_BASE_PATH, 'lr')\n",
    "\n",
    "NCAR_FILES_EXTENSION = '.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c5ce8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:38:29.547871Z",
     "iopub.status.busy": "2024-11-06T00:38:29.547079Z",
     "iopub.status.idle": "2024-11-06T00:38:29.558345Z",
     "shell.execute_reply": "2024-11-06T00:38:29.556161Z",
     "shell.execute_reply.started": "2024-11-06T00:38:29.547820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import packages\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import zarr\n",
    "import itertools\n",
    "import contextlib\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from zarr.storage import DirectoryStore\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14547e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:38:30.175794Z",
     "iopub.status.busy": "2024-11-06T00:38:30.174995Z",
     "iopub.status.idle": "2024-11-06T00:38:30.192352Z",
     "shell.execute_reply": "2024-11-06T00:38:30.190511Z",
     "shell.execute_reply.started": "2024-11-06T00:38:30.175738Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "parameters\n",
    "\"\"\"\n",
    "dataset_title = 'stsabl2048high'\n",
    "store_path = f\"/home/idies/workspace/turbulence-ceph-staging/sciserver-turbulence/{dataset_title}/{dataset_title}.zarr\"\n",
    "store = DirectoryStore(store_path, dimension_separator = '/')\n",
    "# time offset from 0 when querying the dataset with giverny.\n",
    "time_offset = 1\n",
    "# offset from the first time chunk to write data into the zarr store on ceph.\n",
    "# if this value = 0, that means time 0 is written to the zarr time chunk folder = 0.\n",
    "# if this value = 1, that means time 0 is written to the zarr time chunk folder = 1. this is needed to keep a placeholder time chunk folder for pchip interpolation\n",
    "# because the precursor time cannot be read by giverny getCutout and will have to be read and written manually.\n",
    "time_ceph_offset = 0\n",
    "# xyz and time dimensions for the full dataset including times for pchip interpolation that cannot be read by giverny.\n",
    "xyzt_dims_full = [2048, 2048, 2048, 105]\n",
    "# xyz and time dimensions for the dataset to be read with giverny.\n",
    "xyzt_dims = [2048, 2048, 2048, 105] \n",
    "# xyz and time chunk sizes for the zarr store on ceph.\n",
    "xyzt_chunk_sizes = [64, 64, 64, 1]\n",
    "# xyz and time dimensions to query in parallel when reading from the legacy stores.\n",
    "xyzt_filedb_file_dims = np.array([512, 512, 512, 1])\n",
    "# use the default stride value of 1 for each axis when using giverny to retrieve a cutout of the data.\n",
    "strides = [1, 1, 1]\n",
    "# map the zarr group variables to the number of values stored for each grid point.\n",
    "zarr_groups = {\n",
    "    'velocity': 3,\n",
    "    'pressure': 1,\n",
    "    'temperature': 1,\n",
    "    'energy': 1\n",
    "}\n",
    "zarr_variables = list(zarr_groups.keys())\n",
    "num_variables = len(zarr_variables)\n",
    "# number of workers to read in parallel.\n",
    "num_workers = 6\n",
    "# maximum number of retries in case of an error.\n",
    "max_retries = 10\n",
    "# output path for writing the report text file.\n",
    "output_path = os.path.join('/home/idies/workspace/turbulence-ceph-staging/sciserver-turbulence/reports/', dataset_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018e21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:38:35.464910Z",
     "iopub.status.busy": "2024-11-06T00:38:35.462388Z",
     "iopub.status.idle": "2024-11-06T00:38:35.514149Z",
     "shell.execute_reply": "2024-11-06T00:38:35.512337Z",
     "shell.execute_reply.started": "2024-11-06T00:38:35.464831Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mcreate_zarr_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxyzt_dims_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxyzt_chunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzarr_groups\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [5], line 9\u001b[0m, in \u001b[0;36mcreate_zarr_store\u001b[0;34m(store, xyzt_dims_full, xyzt_chunk_sizes, zarr_groups)\u001b[0m\n\u001b[1;32m      6\u001b[0m dt \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mnewbyteorder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# create the zarr store.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# create the zarr group for each variable.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable_name \u001b[38;5;129;01min\u001b[39;00m zarr_groups:\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/site-packages/zarr/hierarchy.py:1286\u001b[0m, in \u001b[0;36mgroup\u001b[0;34m(store, overwrite, chunk_store, cache_attrs, synchronizer, path, zarr_version)\u001b[0m\n\u001b[1;32m   1283\u001b[0m     requires_init \u001b[38;5;241m=\u001b[39m overwrite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contains_group(store, path)\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_init:\n\u001b[0;32m-> 1286\u001b[0m     \u001b[43minit_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m               \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Group(store, read_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, chunk_store\u001b[38;5;241m=\u001b[39mchunk_store,\n\u001b[1;32m   1290\u001b[0m              cache_attrs\u001b[38;5;241m=\u001b[39mcache_attrs, synchronizer\u001b[38;5;241m=\u001b[39msynchronizer, path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   1291\u001b[0m              zarr_version\u001b[38;5;241m=\u001b[39mzarr_version)\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/site-packages/zarr/storage.py:633\u001b[0m, in \u001b[0;36minit_group\u001b[0;34m(store, overwrite, path, chunk_store)\u001b[0m\n\u001b[1;32m    630\u001b[0m     store[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzarr.json\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39m_metadata_class\u001b[38;5;241m.\u001b[39mencode_hierarchy_metadata(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# initialise metadata\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m \u001b[43m_init_group_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m store_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# TODO: Should initializing a v3 group also create a corresponding\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;66;03m#       empty folder under data/root/? I think probably not until there\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;66;03m#       is actual data written there.\u001b[39;00m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/site-packages/zarr/storage.py:657\u001b[0m, in \u001b[0;36m_init_group_metadata\u001b[0;34m(store, overwrite, path, chunk_store)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overwrite:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m store_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;66;03m# attempt to delete any pre-existing items in store\u001b[39;00m\n\u001b[0;32m--> 657\u001b[0m         \u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m             rmdir(chunk_store, path)\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/site-packages/zarr/storage.py:182\u001b[0m, in \u001b[0;36mrmdir\u001b[0;34m(store, path)\u001b[0m\n\u001b[1;32m    179\u001b[0m store_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(store, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store_version\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(store, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmdir\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m store\u001b[38;5;241m.\u001b[39mis_erasable():  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# pass through\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# slow version, delete one key at a time\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m store_version \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/site-packages/zarr/storage.py:1218\u001b[0m, in \u001b[0;36mDirectoryStore.rmdir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1216\u001b[0m     dir_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path, store_path)\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dir_path):\n\u001b[0;32m-> 1218\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/shutil.py:734\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(fd)):\n\u001b[0;32m--> 734\u001b[0m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    736\u001b[0m             os\u001b[38;5;241m.\u001b[39mclose(fd)\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/shutil.py:667\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(dirfd)):\n\u001b[0;32m--> 667\u001b[0m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    669\u001b[0m             os\u001b[38;5;241m.\u001b[39mclose(dirfd)\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/shutil.py:667\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(dirfd)):\n\u001b[0;32m--> 667\u001b[0m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    669\u001b[0m             os\u001b[38;5;241m.\u001b[39mclose(dirfd)\n",
      "    \u001b[0;31m[... skipping similar frames: _rmtree_safe_fd at line 667 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/shutil.py:667\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(dirfd)):\n\u001b[0;32m--> 667\u001b[0m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    669\u001b[0m             os\u001b[38;5;241m.\u001b[39mclose(dirfd)\n",
      "File \u001b[0;32m~/mambaforge/envs/py39/lib/python3.9/shutil.py:688\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 688\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopfd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    690\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39munlink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create the zarr store\n",
    "\"\"\"\n",
    "def create_zarr_store(store, xyzt_dims_full, xyzt_chunk_sizes, zarr_groups):\n",
    "    dt = np.dtype(np.float32)\n",
    "    dt = dt.newbyteorder('<')\n",
    "    \n",
    "    # create the zarr store.\n",
    "    root = zarr.group(store = store, overwrite = True, synchronizer = None)\n",
    "    \n",
    "    # create the zarr group for each variable.\n",
    "    for variable_name in zarr_groups:\n",
    "        zarr_dims = zarr_groups[variable_name]\n",
    "        \n",
    "        zarr_group = root.zeros(variable_name, shape = (xyzt_dims_full[3], xyzt_dims_full[2], xyzt_dims_full[1], xyzt_dims_full[0], zarr_dims), \n",
    "                                chunks = (xyzt_chunk_sizes[3], xyzt_chunk_sizes[2], xyzt_chunk_sizes[1], xyzt_chunk_sizes[0], zarr_dims), \n",
    "                                dtype = dt, compressor = None)\n",
    "    \n",
    "    print('zarr store created.')\n",
    "    print('-')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "create_zarr_store(store, xyzt_dims_full, xyzt_chunk_sizes, zarr_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0e62f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:38:38.840882Z",
     "iopub.status.busy": "2024-11-06T00:38:38.840021Z",
     "iopub.status.idle": "2024-11-06T00:38:39.403581Z",
     "shell.execute_reply": "2024-11-06T00:38:39.401065Z",
     "shell.execute_reply.started": "2024-11-06T00:38:38.840827Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "def merge_velocities(transposed_ds, chunk_size_base=64):\n",
    "    \"\"\"\n",
    "        Merge the 3 velocity components/directions - such merging\n",
    "        exhibits faster 3-component reads. This is a Dask lazy\n",
    "         computation\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge Velocities into 1\n",
    "    b = da.stack([transposed_ds['u'], transposed_ds['v'], transposed_ds['w']], axis=3)\n",
    "    b = b.squeeze()  # It should be (2048, 2048, 2048, 3, 1) before this. Use (2048, 2048, 2048, 3)\n",
    "    # Make into correct chunk sizes\n",
    "    b = b.rechunk((chunk_size_base, chunk_size_base, chunk_size_base, 3))  # Dask chooses (64,64,64,1)\n",
    "    result = transposed_ds.drop_vars(['u', 'v', 'w'])  # Drop individual velocities\n",
    "\n",
    "    # Add joined velocity to original group\n",
    "    # Can't make the dim name same as scalars\n",
    "    result['velocity'] = xr.DataArray(b, dims=(\n",
    "        'nnz', 'nny', 'nnx', 'velocity component (xyz)'))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da7102-a14c-4fb1-9b3d-10e9250a9c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:38:39.407083Z",
     "iopub.status.busy": "2024-11-06T00:38:39.406569Z",
     "iopub.status.idle": "2024-11-06T00:38:42.010568Z",
     "shell.execute_reply": "2024-11-06T00:38:42.008560Z",
     "shell.execute_reply.started": "2024-11-06T00:38:39.407033Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from giverny.turbulence_dataset import *\n",
    "from giverny.turbulence_toolkit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f961ab-cd78-4565-b66e-4f6fb5f09bf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:38:42.014412Z",
     "iopub.status.busy": "2024-11-06T00:38:42.013180Z",
     "iopub.status.idle": "2024-11-06T00:38:42.026057Z",
     "shell.execute_reply": "2024-11-06T00:38:42.024197Z",
     "shell.execute_reply.started": "2024-11-06T00:38:42.014359Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def select_file(folder_path, timestep, file_acronym='jhf'):\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    for filename in file_list:   \n",
    "        # Extract the number from the filename using a more specific regular expression\n",
    "        match = re.search(file_acronym+r'\\.(\\d+)\\.nc', filename)\n",
    "        if match:\n",
    "            file_timestep = int(match.group(1))\n",
    "            if file_timestep == timestep:\n",
    "                # Return full paths\n",
    "                return os.path.join(folder_path, filename)\n",
    "    # If no file is found, raise an exception\n",
    "    raise FileNotFoundError(f\"No file found for timestep {timestep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcade330",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T00:38:44.239339Z",
     "iopub.status.busy": "2024-11-06T00:38:44.238570Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read JHTDB datasets from fileDB using giverny and then write to a zarr store on ceph\n",
    "\"\"\"\n",
    "import xarray as xr\n",
    "import traceback\n",
    "import dask\n",
    "\n",
    "# open the zarr store using DirectoryStore.\n",
    "root = zarr.open(store, mode = 'a')\n",
    "\n",
    "def process_cube(coords):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        TODO : Ariel change code here for reading in the new NCAR data without giverny.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize dataset and open zarr store.\n",
    "        # TODO Need to replace the hard-coded file with timestep-based filename\n",
    "        timestep = coords[3]\n",
    "        # print(\"full filepath: \", select_file(NCAR_JHF_LR_PATH, timestep))\n",
    "        data_xr = xr.open_dataset(select_file(NCAR_JHF_LR_PATH, timestep),\n",
    "                                  chunks={'nnz': xyzt_filedb_file_dims[2],\n",
    "                                          'nny': xyzt_filedb_file_dims[1],\n",
    "                                          'nnx': xyzt_filedb_file_dims[0]})\n",
    "    \n",
    "        assert isinstance(data_xr['e'].data, dask.array.core.Array)\n",
    "        \n",
    "                # Unpack cube coordinates\n",
    "        i, j, k = coords[:3]  # i: X-index, j: Y-index, k: Z-index\n",
    "\n",
    "        # Chunk sizes for each dimension\n",
    "        chunk_sizes = {\n",
    "            'nnx': xyzt_filedb_file_dims[0],\n",
    "            'nny': xyzt_filedb_file_dims[1],\n",
    "            'nnz': xyzt_filedb_file_dims[2],\n",
    "        }\n",
    "\n",
    "        # Construct the selection dictionary\n",
    "        selection = {\n",
    "            'nnz': slice(k * chunk_sizes['nnz'], (k + 1) * chunk_sizes['nnz']),\n",
    "            'nny': slice(j * chunk_sizes['nny'], (j + 1) * chunk_sizes['nny']),\n",
    "            'nnx': slice(i * chunk_sizes['nnx'], (i + 1) * chunk_sizes['nnx']),\n",
    "        }\n",
    "        \n",
    "        # 512-subcube to manage memory constraints\n",
    "        data_subcube = data_xr.isel(selection)\n",
    "        \n",
    "        # Group 3 velocity components together\n",
    "        merged_velocity = merge_velocities(data_subcube, chunk_size_base=xyzt_chunk_sizes[0])\n",
    "\n",
    "        # Add an extra dimension to the data to match isotropic8192\n",
    "        for v in ['e','p','t']:\n",
    "            merged_velocity[v] = merged_velocity[v].expand_dims({'extra_dim': [1]}).drop_vars('extra_dim')\n",
    "            # Put the extra dimension in the back\n",
    "            merged_velocity[v] = merged_velocity[v].transpose('nnz', 'nny', 'nnx', 'extra_dim')\n",
    "        \n",
    "\n",
    "        # Rename variables\n",
    "        merged_velocity = merged_velocity.rename({'e': 'energy', 't': 'temperature', 'p': 'pressure'})\n",
    "        \n",
    "        dataset = merged_velocity\n",
    "\n",
    "        x, y, z = [coord * xyzt_filedb_file_dims[index] + 1 for index, coord in enumerate(coords[:3])]\n",
    "        time = coords[3]\n",
    "        ranges = [[x, x + xyzt_filedb_file_dims[0] - 1],\n",
    "                  [y, y + xyzt_filedb_file_dims[1] - 1],\n",
    "                  [z, z + xyzt_filedb_file_dims[2] - 1]]\n",
    "        \n",
    "        \n",
    "\n",
    "        # Needed for Mike's getCutout below\n",
    "        dataset.attrs['dataset_title'] = 'stsabl2048low'\n",
    "            \n",
    "        variable_data = []\n",
    "        for zarr_variable in zarr_variables:\n",
    "            variable_data.append(dataset[zarr_variable].values)\n",
    "            # data_array = dataset[zarr_variable].values\n",
    "            # data_array = np.expand_dims(data_array, axis=0)  # Adds time dimension at axis 0\n",
    "            # variable_data.append(data_array)\n",
    "\n",
    "            \n",
    "        \"\"\"\n",
    "        TODO : End of code for reading in data.\n",
    "        \"\"\"\n",
    "\n",
    "        def save_store(giverny_cube, variable_name):\n",
    "            root[variable_name][time + time_ceph_offset,\n",
    "                                z - 1 : z + xyzt_filedb_file_dims[2] - 1,\n",
    "                                y - 1 : y + xyzt_filedb_file_dims[1] - 1,\n",
    "                                x - 1 : x + xyzt_filedb_file_dims[0] - 1, :] = giverny_cube\n",
    "            \n",
    "        def verify_copy(giverny_cube, variable_name):\n",
    "            ceph_cube = root[variable_name][time + time_ceph_offset,\n",
    "                                            z - 1 : z + xyzt_filedb_file_dims[2] - 1,\n",
    "                                            y - 1 : y + xyzt_filedb_file_dims[1] - 1,\n",
    "                                            x - 1 : x + xyzt_filedb_file_dims[0] - 1, :]\n",
    "            \n",
    "            if np.all(giverny_cube == ceph_cube):\n",
    "                return \"valid copy\"\n",
    "            else:\n",
    "                return \"corrupt copy\"\n",
    "\n",
    "        # Save the variables in parallel.\n",
    "        with ThreadPoolExecutor(num_variables) as p:\n",
    "            list(p.map(save_store, variable_data, zarr_variables))\n",
    "            \n",
    "        # Verify that the copies are not corrupt.\n",
    "        with ThreadPoolExecutor(num_variables) as p:\n",
    "            verified = list(p.map(verify_copy, variable_data, zarr_variables))\n",
    "\n",
    "        if any([message == \"corrupt copy\" for message in verified]) or len(verified) != num_variables:\n",
    "            return f\"error processing cube at {coords}: verification failed\"\n",
    "        else:\n",
    "            return f\"successfully processed cube at {coords}\"\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        raise Exception(f\"error processing cube at {coords}: {type(e).__name__}, {e}\\n{tb}\")\n",
    "        \n",
    "# Commented out the suppress_stderr context manager\n",
    "# @contextlib.contextmanager\n",
    "# def suppress_stderr():\n",
    "#     with open(os.devnull, 'w') as devnull:\n",
    "#         with contextlib.redirect_stderr(devnull):\n",
    "#             yield\n",
    "\n",
    "# Write the report file.\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "    \n",
    "# Process cubes in parallel.\n",
    "cube_coords = list(itertools.product(*[range(dim // chunk) for dim, chunk in zip(xyzt_dims[:3], xyzt_filedb_file_dims[:3])]))\n",
    "    \n",
    "current_time_pbar = tqdm(total=len(cube_coords), desc=\"chunks completed\", leave=False)\n",
    "    \n",
    "with open(os.path.join(output_path, f\"{dataset_title}_report-success.txt\"), \"w\") as f_success:\n",
    "    with open(os.path.join(output_path, f\"{dataset_title}_report-error.txt\"), \"w\") as f_error:\n",
    "        for time in tqdm(range(xyzt_dims[-1]), total=xyzt_dims[-1], desc=\"time\"):\n",
    "            correct_flag = False\n",
    "            retries = 0\n",
    "            while not correct_flag and retries <= max_retries:\n",
    "                retries += 1\n",
    "                \n",
    "                # Update cube_coords with the time.\n",
    "                cube_coords_time = [(x, y, z, time) for x, y, z in cube_coords]\n",
    "                \n",
    "                results = []\n",
    "                try:\n",
    "                    with ProcessPoolExecutor(num_workers) as executor:\n",
    "                        # Removed suppress_stderr()\n",
    "                        for result in executor.map(process_cube, cube_coords_time):\n",
    "                            results.append(result)\n",
    "                            current_time_pbar.update(1)\n",
    "                            \n",
    "                            if \"error processing cube\" in result:\n",
    "                                break\n",
    "                    \n",
    "                    # Reset the chunk progress bar.\n",
    "                    current_time_pbar.reset()\n",
    "                    \n",
    "                    print('\\r' + ' ' * 100 + '\\r', end='', flush=True)\n",
    "                    if any([\"error processing cube\" in message for message in results]) or results == []:\n",
    "                        if retries > max_retries:\n",
    "                            output_str = f\"error consolidating data for time = {time}\\n\"\n",
    "                            f_error.write(output_str)\n",
    "                            f_error.flush()\n",
    "\n",
    "                            print(f\"\\rerror consolidating data for time = {time}\", end='')\n",
    "                        else:\n",
    "                            print(f\"\\rretrying (n = {retries}) time = {time}\", end='')\n",
    "                            continue\n",
    "                    else:\n",
    "                        output_str = f\"successfully consolidated data for time = {time}\\n\"\n",
    "                        f_success.write(output_str)\n",
    "                        f_success.flush()\n",
    "\n",
    "                        print(f\"\\rsuccessfully consolidated data for time = {time}\", end='')\n",
    "\n",
    "                        correct_flag = True\n",
    "                except Exception as e:\n",
    "                    tb = traceback.format_exc()\n",
    "                    print('\\r' + ' ' * 100 + '\\r', end='', flush=True)\n",
    "                    \n",
    "                    # Reset the chunk progress bar.\n",
    "                    current_time_pbar.reset()\n",
    "                    \n",
    "                    error_message = f\"code exception when consolidating data for time = {time}: {type(e).__name__}, {e}\\n{tb}\\n\"\n",
    "                    f_error.write(error_message)\n",
    "                    f_error.flush()\n",
    "\n",
    "                    print(f\"\\rcode exception when consolidating data for time = {time}: {e}\", end='')\n",
    "                    if retries > max_retries:\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"\\rretrying (n = {retries}) time = {time}\", end='')\n",
    "                        continue\n",
    "\n",
    "print('\\n-')\n",
    "print(\"completed zarr consolidation, check report files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393d43c-e144-4a4a-ad94-ccb0d28e05e7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T00:54:41.809660Z",
     "iopub.status.idle": "2024-11-12T00:54:41.810373Z",
     "shell.execute_reply": "2024-11-12T00:54:41.809981Z",
     "shell.execute_reply.started": "2024-11-12T00:54:41.809947Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "timestep = 0\n",
    "\n",
    "data_xr = xr.open_dataset(select_file(NCAR_JHF_LR_PATH, timestep),\n",
    "                                  chunks={'nnz': xyzt_filedb_file_dims[2],\n",
    "                                          'nny': xyzt_filedb_file_dims[1],\n",
    "                                          'nnx': xyzt_filedb_file_dims[0]})\n",
    "    \n",
    "assert isinstance(data_xr['e'].data, dask.array.core.Array)\n",
    "\n",
    "        # Unpack cube coordinates\n",
    "# i, j, k = coords[:3]  # i: X-index, j: Y-index, k: Z-index\n",
    "\n",
    "# Chunk sizes for each dimension\n",
    "chunk_sizes = {\n",
    "    'nnx': xyzt_filedb_file_dims[0],\n",
    "    'nny': xyzt_filedb_file_dims[1],\n",
    "    'nnz': xyzt_filedb_file_dims[2],\n",
    "}\n",
    "\n",
    "# Construct the selection dictionary\n",
    "# selection = {\n",
    "#     'nnz': slice(k * chunk_sizes['nnz'], (k + 1) * chunk_sizes['nnz']),\n",
    "#     'nny': slice(j * chunk_sizes['nny'], (j + 1) * chunk_sizes['nny']),\n",
    "#     'nnx': slice(i * chunk_sizes['nnx'], (i + 1) * chunk_sizes['nnx']),\n",
    "# }\n",
    "\n",
    "# 512-subcube to manage memory constraints\n",
    "data_subcube = data_xr#.isel(selection)\n",
    "\n",
    "# Group 3 velocity components together\n",
    "merged_velocity = merge_velocities(data_subcube, chunk_size_base=xyzt_chunk_sizes[0])\n",
    "\n",
    "# Add an extra dimension to the data to match isotropic8192\n",
    "for v in ['e','p','t']:\n",
    "    merged_velocity[v] = merged_velocity[v].expand_dims({'extra_dim': [1]}).drop_vars('extra_dim')\n",
    "    # Put the extra dimension in the back\n",
    "    merged_velocity[v] = merged_velocity[v].transpose('nnz', 'nny', 'nnx', 'extra_dim')\n",
    "\n",
    "\n",
    "# Rename variables\n",
    "merged_velocity = merged_velocity.rename({'e': 'energy', 't': 'temperature', 'p': 'pressure'})\n",
    "\n",
    "dataset = merged_velocity\n",
    "\n",
    "# x, y, z = [coord * xyzt_filedb_file_dims[index] + 1 for index, coord in enumerate(coords[:3])]\n",
    "# time = coords[3]\n",
    "# ranges = [[x, x + xyzt_filedb_file_dims[0] - 1],\n",
    "#           [y, y + xyzt_filedb_file_dims[1] - 1],\n",
    "#           [z, z + xyzt_filedb_file_dims[2] - 1]]\n",
    "\n",
    "\n",
    "\n",
    "# Needed for Mike's getCutout below\n",
    "dataset.attrs['dataset_title'] = 'stsabl2048low'\n",
    "\n",
    "variable_data = []\n",
    "for zarr_variable in zarr_variables:\n",
    "    variable_data.append(dataset[zarr_variable].values)\n",
    "    # data_array = dataset[zarr_variable].values\n",
    "    # data_array = np.expand_dims(data_array, axis=0)  # Adds time dimension at axis 0\n",
    "    # variable_data.append(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042f6a4-bd6e-4e7c-b35d-77e5fe7743a4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-12T00:54:41.812705Z",
     "iopub.status.idle": "2024-11-12T00:54:41.813364Z",
     "shell.execute_reply": "2024-11-12T00:54:41.813042Z",
     "shell.execute_reply.started": "2024-11-12T00:54:41.812993Z"
    }
   },
   "outputs": [],
   "source": [
    "variable_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a55ea-1c9d-479e-a792-e98a241b0356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
